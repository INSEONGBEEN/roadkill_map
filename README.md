# roadkill_map
import requests
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
import tensorflow as tf
import torch
import folium

print("âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ")

# ê³µê³µë°ì´í„°í¬í„¸ api ì—°ë™
import requests
import pandas as pd
# ë°ì´í„° ìˆ˜ì§‘
# â–¶ï¸ ì—°ë„ë³„ API URL ë¦¬ìŠ¤íŠ¸
apis = {
    '2019': 'https://api.odcloud.kr/api/15045544/v1/uddi:64ae17e6-d3b2-46d1-be8d-9e8397aa70df',
    '2020': 'https://api.odcloud.kr/api/15045544/v1/uddi:7f1ff5b3-01a4-4d16-9111-c17fa3f34c14',
    '2021': 'https://api.odcloud.kr/api/15045544/v1/uddi:574ad30d-5e70-4ffb-a3e7-debdeb47925b',
    '2022': 'https://api.odcloud.kr/api/15045544/v1/uddi:362b14d7-6360-4070-8f7b-a2261a8a3e0d',
    '2023': 'https://api.odcloud.kr/api/15045544/v1/uddi:36c7c494-4896-4a76-bc4c-f9133762c595'  # 2023ìš© URL ë„£ê¸°
}

# â–¶ï¸ ì¸ì¦í‚¤ (ë””ì½”ë”©ëœ ìƒíƒœë¡œ!)
SERVICE_KEY = 'your_api_key'

# â–¶ï¸ ì „ì²´ ë°ì´í„° ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
all_dataframes = []

# â–¶ï¸ ì—°ë„ë³„ë¡œ ìš”ì²­ ë°˜ë³µ
for year, api_url in apis.items():
    print(f'ğŸ“¥ {year}ë…„ë„ ë°ì´í„° ìš”ì²­ ì¤‘...')
    params = {
        'serviceKey': SERVICE_KEY,
        'page': 1,
        'perPage': 1000  # í•„ìš”ì‹œ í˜ì´ì§€ë„¤ì´ì…˜ ì¶”ê°€ ê°€ëŠ¥
    }
    response = requests.get(api_url, params=params)
    if response.status_code == 200:
        items = response.json()['data']
        df = pd.DataFrame(items)
        df['ë…„ë„'] = year  # âœ… ì—°ë„ ì»¬ëŸ¼ ì¶”ê°€
        all_dataframes.append(df)
        print(f'âœ… {year}ë…„ë„ ë°ì´í„° {len(df)}ê±´ ì™„ë£Œ')
    else:
        print(f'âŒ {year}ë…„ë„ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}')
        print(response.text)

# â–¶ï¸ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
df_all = pd.concat(all_dataframes, ignore_index=True)

# â–¶ï¸ ê²°ê³¼ í™•ì¸
print(df_all.shape)
print(df_all.head())

# â–¶ï¸ CSVë¡œ ì €ì¥ (ì„ íƒ)
df_all.to_csv('roadkill_2019_2023.csv', index=False)

# ë°ì´í„° ì „ì²˜ë¦¬
# ë…„ë„ë³„ ë°ì´í„° ë³‘í•©ì‹œ ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì‚­ì œ
df_all = df_all.drop(['ë°˜ê¸°', 'ì‚¬ê³ ìœ¨', 'ë…¸ì„ ì½”ë“œ', 'ë°© í–¥','ë°©í–¥'], axis=1)
#ë…„ë„ 664ê°œ ë°ì´í„° ì‚½ì… ì´ê±´ ë…„ë„ë³„ ë°ì´í„° í•©ì¹˜ëŠ” ê³¼ì •ì—ì„œ ì¼ë¶€ ë¹ˆ ë°ì´í„°ë‚˜ ê²°ì¸¡ì¹˜ë§Œ ìˆëŠ” í–‰ì´ ë“¤ì–´ê°”ì„ ê°€ëŠ¥ì„±
#ê²°ì¸¡ì¹˜ë§Œ ìˆëŠ” í–‰ ì°¾ê¸°
empty_rows = df[df.isnull().sum(axis=1) == (len(df.columns)-1)]  # ë…„ë„ë§Œ ìˆê³  ë‚˜ë¨¸ì§€ ì „ë¶€ ê²°ì¸¡
print(empty_rows)
empty_rows = df[df['ë°œìƒê±´ìˆ˜'].isnull() & df['ê²½ë„'].isnull() & df['ìœ„ë„'].isnull()]
print(empty_rows)

# ë¨¸ì‹ ëŸ¬ë‹/ì‹œê°í™”ë¥¼ ìœ„í•´ object íƒ€ì… -> ìˆ˜ì¹˜í˜• ë³€í™˜
df_all['ë°œìƒê±´ìˆ˜'] = pd.to_numeric(df_all['ë°œìƒê±´ìˆ˜'], errors='coerce')
df_all['ìœ„ë„'] = pd.to_numeric(df_all['ìœ„ë„'], errors='coerce')
df_all['ê²½ë„'] = pd.to_numeric(df_all['ê²½ë„'], errors='coerce')

# EDA ë¶€ë¶„
# ë¡œë“œí‚¬ ë°œìƒê±´ìˆ˜ ë¶„í¬(íˆìŠ¤í† ê·¸ë¨)
plt.figure(figsize=(8, 5))
df_all['ë°œìƒê±´ìˆ˜'].hist(bins=15)
plt.title('ë¡œë“œí‚¬ ë°œìƒê±´ìˆ˜ ë¶„í¬')
plt.xlabel('ë°œìƒê±´ìˆ˜')
plt.ylabel('ë¹ˆë„')
plt.show()

# ì—°ë„ë³„ ë¡œë“œí‚¬ ë°œìƒê±´ìˆ˜ í•©ê³„
df_all.groupby('ë…„ë„')['ë°œìƒê±´ìˆ˜'].sum().plot(kind='bar', figsize=(8, 5))
plt.title('ì—°ë„ë³„ ë¡œë“œí‚¬ ë°œìƒê±´ìˆ˜ í•©ê³„')
plt.xlabel('ë…„ë„')
plt.ylabel('ì´ ë°œìƒê±´ìˆ˜')
plt.show()

# ë³¸ë¶€ë³„ ë¡œë“œí‚¬ ë°œìƒê±´ìˆ˜ í•©ê³„
df_all.groupby('ë³¸ë¶€ëª…')['ë°œìƒê±´ìˆ˜'].sum().sort_values(ascending=False).plot(kind='bar', figsize=(10, 5))
plt.title('ë³¸ë¶€ë³„ ë¡œë“œí‚¬ ë°œìƒê±´ìˆ˜')
plt.ylabel('ì´ ë°œìƒê±´ìˆ˜')
plt.xlabel('ë³¸ë¶€ëª…')
plt.show()

# ë…¸ì„ ë³„ ìƒìœ„ 10ê°œ ë‹¤ë°œêµ¬ê°„
df_all.groupby('ë…¸ì„ ëª…')['ë°œìƒê±´ìˆ˜'].sum().sort_values(ascending=False).head(10).plot(kind='bar', figsize=(10, 5))
plt.title('ë…¸ì„ ë³„ ìƒìœ„ 10ê°œ ë¡œë“œí‚¬ ë‹¤ë°œêµ¬ê°„')
plt.ylabel('ì´ ë°œìƒê±´ìˆ˜')
plt.xlabel('ë…¸ì„ ëª…')
plt.show()

# ìœ„ê²½ë„ ì‚°ì ë„(ë¡œë“œí‚¬ ë°œìƒ ìœ„ì¹˜)
plt.figure(figsize=(8, 8))
plt.scatter(df_all['ê²½ë„'], df_all['ìœ„ë„'], s=df_all['ë°œìƒê±´ìˆ˜']*5, alpha=0.5)
plt.title('ë¡œë“œí‚¬ ë°œìƒ ìœ„ì¹˜ (ê²½ë„ vs ìœ„ë„)')
plt.xlabel('ê²½ë„')
plt.ylabel('ìœ„ë„')
plt.show()

# ë¨¸ì‹ ëŸ¬ë‹ ëœë¤í¬ë ˆìŠ¤íŠ¸ ì±„íƒ >> ì„¤ëª…ë ¥ì´ 6% ë°–ì— ì•ˆë¨.. > ë³´ì™„ í•„ìš”

# ë…ë¦½,ì¢…ì†ë³€ìˆ˜ ì„¤ì •
feature_cols = ['ìœ„ë„', 'ê²½ë„']
X = df_all[feature_cols]
y = df_all['ë°œìƒê±´ìˆ˜']
# í•™ìŠµ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• 
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ëœë¤í¬ë ˆìŠ¤íŠ¸ íšŒê·€
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# ì˜ˆì¸¡
y_pred = model.predict(X_test)

# í‰ê°€
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f'RMSE: {rmse:.3f}')
print(f'R2 Score: {r2:.3f}')

# ì¼ë‹¨ ëœë¤í¬ë ˆìŠ¤íŠ¸ ë¶„ë¥˜ ì ìš© > ëª¨ë¸ì´ ì •í™•í•˜ì§€ì•ŠìŒ... > ì´ì§„ë¶„ë¥˜ë¡œ ì„ íƒí•¨
# ë°œìƒê±´ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì €ìœ„í—˜ / ì¤‘ìœ„í—˜ / ê³ ìœ„í—˜ 3ê°œ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜

# íƒ€ê²Ÿ ë³€ìˆ˜ ë§Œë“¤ê¸°
def classify_risk(val):
    if val <= 3:
        return 'ì €ìœ„í—˜'
    elif val <= 7:
        return 'ì¤‘ìœ„í—˜'
    else:
        return 'ê³ ìœ„í—˜'

df_all['ìœ„í—˜ë“±ê¸‰'] = df_all['ë°œìƒê±´ìˆ˜'].apply(classify_risk)

X = df_all[['ìœ„ë„', 'ê²½ë„']]
y = df_all['ìœ„í—˜ë“±ê¸‰']

#í•™ìŠµ ë°ì´í„° ë¶„í• 
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ëœë¤í¬ë ˆìŠ¤íŠ¸ ë¶„ë¥˜ê¸°
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

clf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
clf.fit(X_train, y_train)

# ì˜ˆì¸¡
y_pred = clf.predict(X_test)

# í‰ê°€
print("Classification Report:\n", classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

# ë‹¤ì¤‘ë¶„ë¥˜ -> ì´ì§„ë¶„ë¥˜ -> ì´ì œ ì¢€ ì •í™•í•´ì§ ROC AUC : 0.942
# ì´ì§„ íƒ€ê²Ÿ ë§Œë“¤ê¸° (ê³ ìœ„í—˜=1, ë‚˜ë¨¸ì§€=0)
df_all['ê³ ìœ„í—˜_ì´ì§„'] = df_all['ìœ„í—˜ë“±ê¸‰'].apply(lambda x: 1 if x == 'ê³ ìœ„í—˜' else 0)
X = df_all[['ìœ„ë„', 'ê²½ë„']]
y = df_all['ê³ ìœ„í—˜_ì´ì§„']

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split

sm = SMOTE(random_state=42)
X_res, y_res = sm.fit_resample(X, y)

# ë‹¤ì‹œ train/test ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(
    X_res, y_res, test_size=0.2, random_state=42, stratify=y_res
)

#ëœë¤í¬ë ˆìŠ¤íŠ¸ ì´ì§„ ë¶„ë¥˜ê¸°
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

clf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
clf.fit(X_train, y_train)

# ì˜ˆì¸¡
y_pred = clf.predict(X_test)

# í‰ê°€
print("Classification Report:\n", classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

# ROC AUC ìŠ¤ì½”ì–´
from sklearn.metrics import roc_auc_score

y_proba = clf.predict_proba(X_test)[:, 1]
auc = roc_auc_score(y_test, y_proba)
print(f'ROC AUC: {auc:.3f}')

# ì‹œê°í™” í•˜ë©´ ì„ì˜ ì§€ì ì— ëŒ€í•œ ë¡œë“œí‚¬ ë°œìƒ ìœ„í—˜ í™•ë¥ ì— ëŒ€í•´ ë‚˜íƒ€ëƒ„
# ì‹ ê·œ êµ¬ê°„ ì˜ˆì¸¡

import folium
import pandas as pd

# ì‹ ê·œ í¬ì¸íŠ¸ ë¦¬ìŠ¤íŠ¸ (ìœ„ë„/ê²½ë„)
new_points = [
    {'name': 'ì¤‘ë¶€ì„ ', 'lat': 37.5465, 'lon': 127.2003},
    {'name': 'ê²½ë¶€ì„ ', 'lat': 36.3504, 'lon': 127.3845},
    {'name': 'ì¤‘ì•™ì„ ', 'lat': 36.8725, 'lon': 128.5507},
    {'name': 'ì„œí•´ì•ˆì„ ', 'lat': 36.7763, 'lon': 126.4504}
]

# ì§€ë„ ì´ˆê¸°í™”
m = folium.Map(location=[36.5, 127.8], zoom_start=7)

# ê° ì‹ ê·œ êµ¬ê°„ì— ëŒ€í•´ ì˜ˆì¸¡ + ë§ˆì»¤ í‘œì‹œ
for point in new_points:
    X_new = pd.DataFrame([[point['lat'], point['lon']]], columns=['ìœ„ë„', 'ê²½ë„'])
    pred = clf.predict(X_new)[0]
    proba = clf.predict_proba(X_new)[0][1]
    risk = 'ê³ ìœ„í—˜' if pred == 1 else 'ì €ìœ„í—˜'
    
    # ìƒ‰ìƒ ì§€ì •
    color = 'red' if pred == 1 else 'green'
    
    # ë§ˆì»¤ ì¶”ê°€
    folium.Marker(
        location=[point['lat'], point['lon']],
        popup=(
            f"{point['name']}<br>"
            f"ì˜ˆì¸¡ ìœ„í—˜ë“±ê¸‰: <b>{risk}</b><br>"
            f"ê³ ìœ„í—˜ í™•ë¥ : {proba:.2f}"
        ),
        icon=folium.Icon(color=color)
    ).add_to(m)

# ì§€ë„ ì¶œë ¥
m

# ì§€ê¸ˆ ë…ë¦½ë³€ìˆ˜ ê²½ìœ„ë„ë§Œ ì‚¬ìš©í•˜ë‹ˆê¹Œ ëœë¤í¬ë ˆìŠ¤íŠ¸ íšŒê·€ê°€ ì„¤ëª…ë ¥ì´ 6%ë°–ì— ì•ˆë¨...
# osmì„ í†µí•´ì„œ í™˜ê²½ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•´ë³´ë ¤ê³  í•¨

# ì˜ˆì‹œë¡œ ì¤‘ë¶€ì„  êµ¬ê°„ì—ì„œ ì„ì˜ ìœ„ì¹˜ ê°€ì ¸ì™€ì„œ ì‹¤í–‰
import osmnx as ox

# ì¢Œí‘œ & ë°˜ê²½ ì„¤ì •
lat, lon = 36.74561896, 127.4691292
radius = 500  # 500m

# ê°€ì ¸ì˜¬ íƒœê·¸ ì •ì˜
tags = {
    'landuse': True,       # í† ì§€ ì´ìš© (forest, farmland ë“±)
    'natural': True,       # ìì—° í™˜ê²½ (water, wood ë“±)
    'highway': True,       # ë„ë¡œ ì •ë³´
    'building': True,      # ê±´ë¬¼ ì •ë³´
    'barrier': True        # ìš¸íƒ€ë¦¬ ë“± (ë¡œë“œí‚¬ ì˜í–¥ í´ ìˆ˜ ìˆìŒ)
}

# í™˜ê²½ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
gdf = ox.features_from_point((lat, lon), tags=tags, dist=radius)

# í™•ì¸í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
columns_to_check = ['geometry', 'landuse', 'natural', 'highway', 'building', 'barrier']

# ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì¶”ì¶œ
existing_cols = [col for col in columns_to_check if col in gdf.columns]

# ì•ˆì „í•˜ê²Œ ì¶œë ¥
if existing_cols:
    print(gdf[existing_cols].head())
else:
    print("í•´ë‹¹ ìœ„ì¹˜ì—ì„œ ê°€ì ¸ì˜¨ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

# ìˆ² ìœ ë¬´
if 'landuse' in gdf.columns:
    forest = int('forest' in gdf['landuse'].dropna().values)
else:
    forest = 0

# ê±´ë¬¼ ê°œìˆ˜
if 'building' in gdf.columns:
    num_buildings = gdf['building'].notnull().sum()
else:
    num_buildings = 0

# motorway ìœ ë¬´
if 'highway' in gdf.columns:
    has_motorway = int('motorway' in gdf['highway'].dropna().values)
else:
    has_motorway = 0

print(f"ìˆ² ìœ ë¬´: {forest}")
print(f"ê±´ë¬¼ ê°œìˆ˜: {num_buildings}")
print(f"ê³ ì†ë„ë¡œ(motorway) ìœ ë¬´: {has_motorway}")

# ë°ì´í„° ì˜ˆì‹œ: df_allì— 'ìœ„ë„'ì™€ 'ê²½ë„' ì»¬ëŸ¼ì´ ìˆë‹¤ê³  ê°€ì •
# df_all = pd.DataFrame(...)

radius = 500  # ë°˜ê²½ 500m
tags = {
    'landuse': True,
    'natural': True,
    'highway': True,
    'building': True,
    'barrier': True
}

env_features = []

for idx, row in df_all.iterrows():
    lat, lon = row['ìœ„ë„'], row['ê²½ë„']
    print(f"[{idx}] ìœ„ì¹˜: ìœ„ë„={lat}, ê²½ë„={lon}")
    
    try:
        gdf = ox.features_from_point((lat, lon), tags=tags, dist=radius)

        # ìˆ² ìœ ë¬´
        forest_tags = ['forest', 'wood']
        has_forest = 0
        if 'landuse' in gdf.columns:
            if any(tag in gdf['landuse'].dropna().values for tag in forest_tags):
                has_forest = 1
        if 'natural' in gdf.columns:
            if any(tag in gdf['natural'].dropna().values for tag in forest_tags):
                has_forest = 1

        # ë†ì§€ ìœ ë¬´
        has_farmland = 0
        if 'landuse' in gdf.columns:
            has_farmland = int('farmland' in gdf['landuse'].dropna().values)

        # ë„ë¡œ íƒ€ì… ê°œìˆ˜
        highway_count = 0
        if 'highway' in gdf.columns:
            highway_count = gdf['highway'].notnull().sum()

        # ê±´ë¬¼ ê°œìˆ˜
        num_buildings = 0
        if 'building' in gdf.columns:
            num_buildings = gdf['building'].notnull().sum()

        # barrier ìœ ë¬´
        has_barrier = 0
        if 'barrier' in gdf.columns:
            has_barrier = int(gdf['barrier'].notnull().any())

        env_features.append({
            'ìˆ²_ìœ ë¬´': has_forest,
            'ë†ì§€_ìœ ë¬´': has_farmland,
            'ë„ë¡œ_ê°œìˆ˜': highway_count,
            'ê±´ë¬¼_ê°œìˆ˜': num_buildings,
            'barrier_ìœ ë¬´': has_barrier
        })

    except Exception as e:
        print(f"Error at index {idx}: {e}")
        env_features.append({
            'ìˆ²_ìœ ë¬´': 0,
            'ë†ì§€_ìœ ë¬´': 0,
            'ë„ë¡œ_ê°œìˆ˜': 0,
            'ê±´ë¬¼_ê°œìˆ˜': 0,
            'barrier_ìœ ë¬´': 0
        })

# í•©ì¹˜ê¸°
env_df = pd.DataFrame(env_features)
df_all = pd.concat([df_all.reset_index(drop=True), env_df], axis=1)

# ì¶”ê°€ëœ í™˜ê²½í”¼ì²˜ë“¤ë¡œ ë‹¤ì‹œ ëœë¤í¬ë ˆìŠ¤íŠ¸ íšŒê·€ë¶„ì„ >> ì„¤ëª…ë ¥ 0.05
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# 1ï¸âƒ£ í”¼ì²˜/íƒ€ê²Ÿ ì¤€ë¹„
feature_cols = ['ìœ„ë„', 'ê²½ë„', 'ìˆ²_ìœ ë¬´', 'ë†ì§€_ìœ ë¬´', 'ë„ë¡œ_ê°œìˆ˜', 'ê±´ë¬¼_ê°œìˆ˜', 'barrier_ìœ ë¬´']
X = df_all[feature_cols]
y = df_all['ë°œìƒê±´ìˆ˜'].astype(float)  # ë°œìƒê±´ìˆ˜ ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜ (ì¤‘ìš”!)

# 2ï¸âƒ£ ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 3ï¸âƒ£ ëª¨ë¸ í•™ìŠµ
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# 4ï¸âƒ£ ì˜ˆì¸¡
y_pred = model.predict(X_test)

# 5ï¸âƒ£ í‰ê°€
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f'âœ… RMSE: {rmse:.3f}')
print(f'âœ… R2 Score: {r2:.3f}')

# XGBoostë„ ì‹œë„! > ì„¤ëª…ë ¥ 0.109
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# 1ï¸âƒ£ í”¼ì²˜/íƒ€ê²Ÿ ì¤€ë¹„
feature_cols = ['ìœ„ë„', 'ê²½ë„', 'ìˆ²_ìœ ë¬´', 'ë†ì§€_ìœ ë¬´', 'ë„ë¡œ_ê°œìˆ˜', 'ê±´ë¬¼_ê°œìˆ˜', 'barrier_ìœ ë¬´']
X = df_all[feature_cols]
y = df_all['ë°œìƒê±´ìˆ˜'].astype(float)

# 2ï¸âƒ£ ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 3ï¸âƒ£ XGBoost íšŒê·€ ëª¨ë¸ ì •ì˜
model = xgb.XGBRegressor(
    n_estimators=100,
    max_depth=4,
    learning_rate=0.1,
    random_state=42
)

# 4ï¸âƒ£ ëª¨ë¸ í•™ìŠµ
model.fit(X_train, y_train)

# 5ï¸âƒ£ ì˜ˆì¸¡
y_pred = model.predict(X_test)

# 6ï¸âƒ£ í‰ê°€
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f'âœ… XGBoost RMSE: {rmse:.3f}')
print(f'âœ… XGBoost R2 Score: {r2:.3f}')

# í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í›„ ëª¨ë¸ í•™ìŠµì‹œí‚¤ë‹ˆ ì„¤ëª…ë ¥ 1.43
# ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìƒˆ ëª¨ë¸ ìƒì„±
best_model = xgb.XGBRegressor(
    n_estimators=50,
    max_depth=4,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.6,
    gamma=0.5,
    random_state=42
)

# train/test ë¶„í• 
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# í•™ìŠµ
best_model.fit(X_train, y_train)

# ì˜ˆì¸¡
y_pred = best_model.predict(X_test)

# í‰ê°€
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f'âœ… ìµœì¢… ëª¨ë¸ RMSE: {rmse:.3f}')
print(f'âœ… ìµœì¢… ëª¨ë¸ R2 Score: {r2:.3f}')

# í˜„ì¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹í•œ ëœë¤í¬ë ˆìŠ¤íŠ¸ íšŒê·€ ì„¤ëª…ë ¥ì´ 14% osmì—ì„œ ì œí•œì†ë„, ì°¨ì„ ìˆ˜, ë„ë¡œí­ ë°ì´í„° ìˆ˜ì§‘í•˜ë ¤ê³  í•¨.
import osmnx as ox
import pandas as pd

# ì˜ˆì‹œ: df_allì— 'ìœ„ë„', 'ê²½ë„' ì»¬ëŸ¼ì´ ìˆë‹¤ê³  ê°€ì •
# df_all = pd.DataFrame(...)

radius = 50  # ë°˜ê²½ 50m (ë„ë¡œ í­ê¹Œì§€ ì¡ê¸° ë•Œë¬¸ì— ì¢ê²Œ ì„¤ì •)
tags = {
    'highway': True,
    'maxspeed': True,
    'lanes': True,
    'width': True  # ë„ë¡œí­ ê°€ì ¸ì˜¤ê¸°
}

# ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
speed_lane_width_data = []

for idx, row in df_all.iterrows():
    lat, lon = row['ìœ„ë„'], row['ê²½ë„']
    print(f"[{idx}] ìœ„ì¹˜: ìœ„ë„={lat}, ê²½ë„={lon}")

    try:
        # ë„ë¡œ í”¼ì²˜ ê°€ì ¸ì˜¤ê¸° (ë¼ì¸ íƒ€ì…ë§Œ ì¶”ì¶œ)
        gdf = ox.features_from_point((lat, lon), tags=tags, dist=radius)
        roads = gdf[gdf.geom_type == 'LineString']
        
        # ì œí•œì†ë„ ì¶”ì¶œ
        maxspeed = None
        if 'maxspeed' in roads.columns:
            maxspeed_values = roads['maxspeed'].dropna().tolist()
            if maxspeed_values:
                val = maxspeed_values[0]
                if isinstance(val, list):
                    val = val[0]
                if isinstance(val, str):
                    val = ''.join([c for c in val if c.isdigit()])
                    maxspeed = int(val) if val.isdigit() else None
                elif isinstance(val, (int, float)):
                    maxspeed = val

        # ì°¨ì„ ìˆ˜ ì¶”ì¶œ
        lanes = None
        if 'lanes' in roads.columns:
            lanes_values = roads['lanes'].dropna().tolist()
            if lanes_values:
                val = lanes_values[0]
                if isinstance(val, list):
                    val = val[0]
                if isinstance(val, str):
                    lanes = int(val) if val.isdigit() else None
                elif isinstance(val, (int, float)):
                    lanes = val

        # ë„ë¡œí­ ì¶”ì¶œ
        width = None
        if 'width' in roads.columns:
            width_values = roads['width'].dropna().tolist()
            if width_values:
                val = width_values[0]
                if isinstance(val, list):
                    val = val[0]
                if isinstance(val, str):
                    width_digits = ''.join([c for c in val if c.isdigit() or c == '.'])
                    width = float(width_digits) if width_digits.replace('.', '', 1).isdigit() else None
                elif isinstance(val, (int, float)):
                    width = val

        speed_lane_width_data.append({
            'ì œí•œì†ë„': maxspeed if maxspeed is not None else 0,
            'ì°¨ì„ ìˆ˜': lanes if lanes is not None else 0,
            'ë„ë¡œí­': width if width is not None else 0
        })

    except Exception as e:
        print(f"Error at index {idx}: {e}")
        speed_lane_width_data.append({
            'ì œí•œì†ë„': 0,
            'ì°¨ì„ ìˆ˜': 0,
            'ë„ë¡œí­': 0
        })

# ê²°ê³¼ ë³‘í•©
speed_lane_width_df = pd.DataFrame(speed_lane_width_data)
df_all = pd.concat([df_all.reset_index(drop=True), speed_lane_width_df], axis=1)

# ì „ì²´ ìƒê´€ê³„ìˆ˜ í™•ì¸ í›„ í•„ìš” ì—†ëŠ” ì»¬ëŸ¼ ì‚­ì œ
corr = df_all[['ìœ„ë„', 'ê²½ë„', 'ìˆ²_ìœ ë¬´', 'ë†ì§€_ìœ ë¬´', 'ë„ë¡œ_ê°œìˆ˜', 'ê±´ë¬¼_ê°œìˆ˜', 'barrier_ìœ ë¬´', 'ì œí•œì†ë„', 'ì°¨ì„ ìˆ˜', 'ë°œìƒê±´ìˆ˜']].corr()

# ë°œìƒê±´ìˆ˜ì™€ ìƒê´€ê´€ê³„ë§Œ ì¶œë ¥
print(corr['ë°œìƒê±´ìˆ˜'].sort_values(ascending=False))

# ìµœì¢… ëª¨ë¸
import xgboost as xgb
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import mean_squared_error, r2_score, make_scorer
import numpy as np

# 1ï¸âƒ£ í”¼ì²˜ & íƒ€ê²Ÿ
feature_cols = ['ìœ„ë„', 'ê²½ë„', 'ìˆ²_ìœ ë¬´', 'barrier_ìœ ë¬´', 'ì œí•œì†ë„', 'ì°¨ì„ ìˆ˜']
X = df_all[feature_cols]
y = df_all['ë°œìƒê±´ìˆ˜'].astype(float)

# 2ï¸âƒ£ ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 3ï¸âƒ£ XGBoost ëª¨ë¸ (ê¸°ë³¸ íŠœë‹ìœ¼ë¡œ ë¨¼ì € ì •ì˜)
model = xgb.XGBRegressor(
    n_estimators=100,
    max_depth=4,
    learning_rate=0.1,
    random_state=42
)

# 4ï¸âƒ£ ëª¨ë¸ í•™ìŠµ
model.fit(X_train, y_train)

# 5ï¸âƒ£ ì˜ˆì¸¡
y_pred = model.predict(X_test)

# 6ï¸âƒ£ í‰ê°€
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f'âœ… XGBoost ê¸°ë³¸ íŠœë‹ RMSE: {rmse:.3f}')
print(f'âœ… XGBoost ê¸°ë³¸ íŠœë‹ R2 Score: {r2:.3f}')

# ğŸš€ ì¶”ê°€: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (RandomizedSearchCV)
param_dist = {
    'n_estimators': [50, 100, 150, 200, 300],
    'max_depth': [3, 4, 5, 6, 8, 10],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'gamma': [0, 0.5, 1, 2]
}

rmse_scorer = make_scorer(mean_squared_error, greater_is_better=False)

random_search = RandomizedSearchCV(
    estimator=xgb.XGBRegressor(random_state=42),
    param_distributions=param_dist,
    n_iter=100,
    scoring=rmse_scorer,
    cv=3,
    verbose=2,
    n_jobs=-1,
    random_state=42
)

# íŠœë‹ í•™ìŠµ
random_search.fit(X, y)

print("âœ… Best Parameters:", random_search.best_params_)
print("âœ… Best RMSE (CV):", np.sqrt(-random_search.best_score_))

# âœ… ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìƒˆ ëª¨ë¸ ìƒì„±
best_params = random_search.best_params_

best_model = xgb.XGBRegressor(
    n_estimators=best_params['n_estimators'],
    max_depth=best_params['max_depth'],
    learning_rate=best_params['learning_rate'],
    subsample=best_params['subsample'],
    colsample_bytree=best_params['colsample_bytree'],
    gamma=best_params['gamma'],
    random_state=42
)

# í•™ìŠµ
best_model.fit(X_train, y_train)

# ì˜ˆì¸¡
y_pred_best = best_model.predict(X_test)

# í‰ê°€
rmse_best = np.sqrt(mean_squared_error(y_test, y_pred_best))
r2_best = r2_score(y_test, y_pred_best)

print(f'âœ… ìµœì¢… íŠœë‹ XGBoost RMSE: {rmse_best:.3f}')
print(f'âœ… ìµœì¢… íŠœë‹ XGBoost R2 Score: {r2_best:.3f}')

# íšŒê·€ëª¨ë¸ë¡œ ì„ì˜ ì§€ì ì— ëŒ€í•œ ë°œìƒê±´ìˆ˜ ì˜ˆì¸¡ ì‹œê°í™”
# ğŸš€ ì‹ ê·œ êµ¬ê°„ ì˜ˆì¸¡ìš© ë°ì´í„°
new_points = [
    {'name': 'ì¤‘ë¶€ì„ ', 'lat': 37.5465, 'lon': 127.2003},
    {'name': 'ê²½ë¶€ì„ ', 'lat': 36.3504, 'lon': 127.3845},
    {'name': 'ì¤‘ì•™ì„ ', 'lat': 36.8725, 'lon': 128.5507},
    {'name': 'ì„œí•´ì•ˆì„ ', 'lat': 36.7763, 'lon': 126.4504}
]

# ğŸ§® ìµœì  ëª¨ë¸ë¡œ ì‹ ê·œ í¬ì¸íŠ¸ ì˜ˆì¸¡
print("\nğŸš¦ ì‹ ê·œ êµ¬ê°„ ì˜ˆì¸¡ ê²°ê³¼ (ìµœì  íŠœë‹ ëª¨ë¸):")
for point in new_points:
    X_new = pd.DataFrame([[
        point['lat'],     # ìœ„ë„
        point['lon'],     # ê²½ë„
        0,                # ìˆ²_ìœ ë¬´ (ê¸°ë³¸ê°’)
        0,                # barrier_ìœ ë¬´ (ê¸°ë³¸ê°’)
        80,               # ì œí•œì†ë„ (ì˜ˆ: 80km/h)
        2                 # ì°¨ì„ ìˆ˜ (ì˜ˆ: 2ì°¨ì„ )
    ]], columns=feature_cols)
    
    predicted = best_model.predict(X_new)[0]
    print(f"{point['name']} (ìœ„ë„: {point['lat']}, ê²½ë„: {point['lon']}) â†’ ì˜ˆìƒ ë¡œë“œí‚¬ ë°œìƒê±´ìˆ˜: {predicted:.2f}")

# ğŸ—ºï¸ ì§€ë„ ì‹œê°í™”
m = folium.Map(location=[36.5, 127.5], zoom_start=7)

for point in new_points:
    X_new = pd.DataFrame([[
        point['lat'],
        point['lon'],
        0,
        0,
        80,
        2
    ]], columns=feature_cols)
    
    predicted = best_model.predict(X_new)[0]
    
    folium.CircleMarker(
        location=[point['lat'], point['lon']],
        radius=10,
        popup=f"{point['name']} - ì˜ˆì¸¡ ë°œìƒê±´ìˆ˜: {predicted:.2f}",
        color='blue',
        fill=True,
        fill_opacity=0.7
    ).add_to(m)

# ì§€ë„ ì¶œë ¥
m

# ë¡œë“œí‚¬ ì˜ˆì¸¡ê°’ì„ í†µí•œ íˆíŠ¸ë§µ ì œì‘ (ìµœì  ëª¨ë¸ ê¸°ë°˜)

# ê·¸ë¦¬ë“œ ë°ì´í„° ìƒì„±
import numpy as np
import pandas as pd

# ëŒ€í•œë¯¼êµ­ ë²”ìœ„ ì„¤ì •
lat_range = np.arange(34.5, 38.5, 0.05)
lon_range = np.arange(126.0, 129.5, 0.05)

grid_points = []

for lat in lat_range:
    for lon in lon_range:
        grid_points.append([lat, lon, 0, 0, 80, 2])  # ìˆ²_ìœ ë¬´=0, barrier_ìœ ë¬´=0, ì œí•œì†ë„=80, ì°¨ì„ ìˆ˜=2

grid_df = pd.DataFrame(grid_points, columns=['ìœ„ë„', 'ê²½ë„', 'ìˆ²_ìœ ë¬´', 'barrier_ìœ ë¬´', 'ì œí•œì†ë„', 'ì°¨ì„ ìˆ˜'])

# ì˜ˆì¸¡ ìˆ˜í–‰ (ìµœì  íŒŒë¼ë¯¸í„° ëª¨ë¸ ì‚¬ìš©)
predictions = best_model.predict(grid_df)
grid_df['ì˜ˆì¸¡ë°œìƒê±´ìˆ˜'] = predictions

# íˆíŠ¸ë§µ ì‹œê°í™”
import folium
from folium.plugins import HeatMap

# ì§€ë„ ì´ˆê¸°í™”
m = folium.Map(location=[36.5, 127.8], zoom_start=7)

# HeatMap ë°ì´í„°: [[ìœ„ë„, ê²½ë„, ê°€ì¤‘ì¹˜]]
heat_data = [[row['ìœ„ë„'], row['ê²½ë„'], row['ì˜ˆì¸¡ë°œìƒê±´ìˆ˜']] for index, row in grid_df.iterrows()]

# HeatMap ì¶”ê°€
HeatMap(
    heat_data,
    radius=12,
    blur=20,
    min_opacity=0.2,
    max_zoom=13
).add_to(m)

# ì§€ë„ ì¶œë ¥
m

# êµí†µëŸ‰ ë°ì´í„° ìˆ˜ì§‘ì„ ìœ„í•´ í•œêµ­ë„ë¡œê³µì‚¬ api ì—°ë™
# ê³µê°„ì •ë³´ë…¸ì„ í˜„í™© ì´ê²ƒë„ apië¡œ ê°€ì ¸ì™€ì•¼í•´? ì§œì¦ë‚˜ë„¤ ã…‹ã…‹
import requests
import pandas as pd

# âœ… ë°œê¸‰ë°›ì€ ì¸ì¦í‚¤
service_key = 'your_api_key'

# âœ… API í˜¸ì¶œ URL
url = 'https://data.ex.co.kr/openapi/roadEtcInfo/spinRouteList'

# âœ… ìš”ì²­ íŒŒë¼ë¯¸í„° ì„¤ì •
params = {
    'key': service_key,
    'type': 'json'
}

# âœ… API í˜¸ì¶œ
response = requests.get(url, params=params)

# âœ… ê²°ê³¼ í™•ì¸
if response.status_code == 200:
    data = response.json()
    items = data.get('list', [])
    print(f'âœ… ë…¸ì„  ìˆ˜ì§‘ ì„±ê³µ: {len(items)}ê±´')
    
    # âœ… ë°ì´í„°í”„ë ˆì„ ë³€í™˜
    df_routes = pd.DataFrame(items)
    print(df_routes.head())
else:
    print(f'âŒ í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code} / {response.text}')

# df_routes: ìƒˆë¡œ ê°€ì ¸ì˜¨ ë…¸ì„ í˜„í™©
# df_all: ê¸°ì¡´ ë°ì´í„°í”„ë ˆì„

df_all = df_all.merge(
    df_routes[['routeNm', 'routeCd', 'routeNo']],
    left_on='ë…¸ì„ ëª…',
    right_on='routeNm',
    how='left'
)

# í•„ìš” ì—†ëŠ” routeNm ì»¬ëŸ¼ ì‚­ì œ
df_all = df_all.drop(columns=['routeNm'])

# ê²°ê³¼ í™•ì¸
print(df_all.head())

# ì•„ë˜ëŠ” ë…¸ì„ ë³„ ì „ì²´ ì˜ì—…ì†Œ êµí†µëŸ‰ api ì—°ë™ ê³¼ì •ì¸ë° ì‹¤ì‹œê°„ ë°ì´í„°ë§Œ ê°€ì ¸ì˜¤ëŠ” ë¬¸ì œ ë°œê²¬í•´ì„œ ì‹¤íŒ¨í•¨..
import requests
import pandas as pd
from datetime import datetime, timedelta

# âœ… ë°œê¸‰ë°›ì€ ì¸ì¦í‚¤
service_key = '3305368534'

# âœ… df_allì— ìˆëŠ” routeNoë§Œ ê³ ìœ  ì¶”ì¶œ
route_nos = df_all['routeNo'].dropna().unique()

# âœ… ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
traffic_data = []

# âœ… ì¡°íšŒí•  ê¸°ê°„ ì„¤ì • (ì˜ˆ: 2023-06-01 ~ 2023-06-30)
start_date = datetime(2023, 6, 1)
end_date = datetime(2023, 6, 30)

# âœ… ë‚ ì§œ ë£¨í”„
current_date = start_date
while current_date <= end_date:
    sum_date = current_date.strftime('%Y%m%d')
    print(f"ğŸ“… {sum_date} ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")

    # âœ… ë…¸ì„  ë£¨í”„
    for route_no in route_nos:
        params = {
            'key': service_key,
            'type': 'json',
            'routeNo': route_no,
            'sumDate': sum_date
        }
        url = 'https://data.ex.co.kr/openapi/trafficapi/trafficRoute'
        response = requests.get(url, params=params)

        if response.status_code == 200:
            data = response.json()
            items = data.get('list', [])
            if items:
                for item in items:
                    item['routeNo'] = route_no
                    item['sumDate'] = sum_date  # ë‚ ì§œë„ ë¶™ì´ê¸°
                    traffic_data.append(item)
            print(f'âœ… {route_no} í˜¸ì¶œ ì„±ê³µ, {len(items)}ê±´ ìˆ˜ì§‘ë¨')
        else:
            print(f'âŒ {route_no} í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}')
    
    current_date += timedelta(days=1)

# âœ… ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
df_traffic = pd.DataFrame(traffic_data)
print(df_traffic.head())
